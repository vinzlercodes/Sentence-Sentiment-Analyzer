{"cells":[{"metadata":{"_uuid":"5f9103d5-e5af-4bde-8948-e7b25cf500bd","_cell_guid":"1249d1bb-5ff4-43e6-83b0-30307b0765af","trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tensorflow_datasets","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tensorflow_datasets\n  Downloading tensorflow_datasets-3.1.0-py3-none-any.whl (3.3 MB)\n\u001b[K     |████████████████████████████████| 3.3 MB 2.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.11.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.18.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.3.1.1)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (3.11.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.14.0)\nCollecting tensorflow-metadata\n  Downloading tensorflow_metadata-0.22.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (19.3.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.9.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (2.23.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.18.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (4.45.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (46.1.3.post20200325)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.51.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.4.5.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\nInstalling collected packages: tensorflow-metadata, tensorflow-datasets\nSuccessfully installed tensorflow-datasets-3.1.0 tensorflow-metadata-0.22.0\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"66f98040-a408-4bee-b7f8-f75fe872b479","_cell_guid":"5c2aea6d-441d-4744-8bb1-b5ce5bfb27ac","trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nimport re #for string operations like cleaning,preproceesing\nimport time #for epoch time\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers #for the model layers\nimport tensorflow_datasets as tfds #for tokenizing the sentences \nfrom bs4 import BeautifulSoup","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"8515f8aa-d7ae-4214-8578-6b986a468db1","_cell_guid":"5d0d0dd3-e6ae-402f-8510-1885274304c6","trusted":true},"cell_type":"code","source":"#Load Data\ncols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n\n#the file has no header. Hence, the columns are not already there and need to be defined and put in\n#Python engine needs to be defined for proper decoding\n#the encoding latin1 is selected as it is the one used for english\ntrain_data = pd.read_csv(\n    \"../input/training.1600000.processed.noemoticon.csv\",\n    header=None,\n    names=cols,\n    engine=\"python\",\n    encoding=\"latin1\"\n)\ntest_data = pd.read_csv(\n    \"../input/testdata.manual.2009.06.14.csv\",\n    header=None,\n    names=cols,\n    engine=\"python\",\n    encoding=\"latin1\"\n)\n#The test dataset has 3 different labels (negative, positive and neutral) while the train dataset \n#has only two so we will not use the test file, and split the train file later by ourselves.\ndata = train_data","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"7f6be9f8-597c-490b-a7e3-014f89fbe956","_cell_guid":"95c8322f-8ccb-445f-8bbe-953092a4020b","trusted":true,"collapsed":true},"cell_type":"code","source":"#Visualizing the data\ntrain_data.head(3)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   sentiment          id                          date     query  \\\n0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n\n              user                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"bc5a6f2a-bd41-440f-9bf5-62525de057d9","_cell_guid":"234c996b-5996-40bf-b418-4671cacff20a","trusted":true},"cell_type":"code","source":"#Pre-Processing\n#Data Cleaning\n\n#we get rid of the columns that we dont need for the model training, we will only be left with the sentiment label\n#and the text column\n#axis is set so we get of the columns and not the rows and axis = 1 is the header column\n#inplace basically updates the data with the deleted columns \ndata.drop([\"id\", \"date\", \"query\", \"user\"], \n          axis=1,\n          inplace=True)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d0f46a92-7fb0-45c7-bf29-473fd04f60ed","_cell_guid":"4e0b7310-2b5f-4d72-a005-a039e80cff1e","trusted":true},"cell_type":"code","source":"#Visualize cleaned data \ndata.head(5)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   sentiment                                               text\n0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1          0  is upset that he can't update his Facebook by ...\n2          0  @Kenichan I dived many times for the ball. Man...\n3          0    my whole body feels itchy and like its on fire \n4          0  @nationwideclass no, it's not behaving at all....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"2be06e3f-ac88-4c76-83ee-f7f47db413bf","_cell_guid":"9aabafa0-4e3f-403e-b4f7-e19c9bff6b65","trusted":true},"cell_type":"code","source":"#since data has been taken from the net there are certain elements in the string that come with it so we need to clean \n#we want to make it into a regular string from the xml format\ndef clean_tweet(tweet):\n    tweet = BeautifulSoup(tweet, \"lxml\").get_text() #we ustilise the beautifulsoup module for intepreting tweet from XML \n    # Removing the @\n    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet) #re.sub() essentially is like a replace function for regex in strings\n    # Removing the URL links\n    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n    # Keeping only letters\n    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet) #replace thats not in the brackets\n    # Removing additional whitespaces\n    tweet = re.sub(r\" +\", ' ', tweet)\n    return tweet","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"48a33ad5-f3bd-41e1-9371-5cff3c9e5d49","_cell_guid":"542e6b2a-9314-41a2-a88d-1628988ca7f4","trusted":true},"cell_type":"code","source":"data_clean = [clean_tweet(tweet) for tweet in data.text]","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"b6eb27b6-bf77-4789-a377-f4c87b9e97f3","_cell_guid":"788cd47f-14e7-48e2-8c3d-0a44d8951a38","trusted":true},"cell_type":"code","source":"#Cleaning the data labels\ndata_labels = data.sentiment.values\ndata_labels[data_labels == 4] = 1 #because the labels took values 0, 2, 4,we make it 0 and 1 (Binary for better reading)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"75d87b81-de5e-4dc2-84cb-edae1aa66adc","_cell_guid":"4b1a6de0-e749-49ff-9114-2a822d720c55","trusted":true},"cell_type":"code","source":"#Tokenization\n#here is where the words are converted to numbers and we use an ecoder module in TF for it\n#target_vocab_size is the number of words we want to see in our vocab and we use 64k (2^16) words \n#this is useful because with words that do not have a number attached to them, the encoder will compose it with words\n#that already exists. most of the time it will be letter by letter but that actually can be quite useful and sometimes\n#it can be powerful with a word that only appears one time in all copies.\n\ntokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n    data_clean, target_vocab_size=2**16\n)\n\ndata_inputs = [tokenizer.encode(sentence) for sentence in data_clean]","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"659d0848-58dd-467e-9f29-49461bae403d","_cell_guid":"3fd9eec4-eb67-47f7-9bdc-6e5f732efee1","trusted":true},"cell_type":"code","source":"#maximum length of sentences\n#Now the length of a sentence is the number of words in it but prev it was the number of characters\n#padding is basically we are making sure that all the inputs are of the same size so if there are varied lengths 0 are added\nMAX_LEN = max([len(sentence) for sentence in data_inputs])\ndata_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n                                                            value=0,\n                                                            padding=\"post\",\n                                                            maxlen=MAX_LEN)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"61104dd3-2823-4c5f-b197-37f949b8659b","_cell_guid":"f34708d7-6e1f-40dd-9616-1114e788119a","trusted":true},"cell_type":"code","source":"#Splitting data into test and train sets\n#our dataset has 1600000 elements with 50-50 of positive and negative sentiments\ntest_idx = np.random.randint(0, 800000, 8000) #negative tweets for testing (first half)\ntest_idx = np.concatenate((test_idx, test_idx+800000)) #containing both positive and negatives","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"acd09668-d07d-4433-b4c3-3e321cefd358","_cell_guid":"69564529-2150-4041-a23b-3eeec1b5e3ad","trusted":true},"cell_type":"code","source":"test_inputs = data_inputs[test_idx] \ntest_labels = data_labels[test_idx]\ntrain_inputs = np.delete(data_inputs, test_idx, axis=0) #removes the testing values\ntrain_labels = np.delete(data_labels, test_idx)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"1caea7d6-3c75-4f09-ac02-94af87fe3286","_cell_guid":"e6ec78c7-ac4e-4bce-8db1-c20bbe6e2020","trusted":true},"cell_type":"code","source":"#CNN MODEL\nclass DCNN(tf.keras.Model):\n    \n    def __init__(self,\n                 vocab_size,\n                 emb_dim=128,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"dcnn\"):\n        super(DCNN, self).__init__(name=name) #call the model from the class we are inheriting from, model initialised\n        \n        self.embedding = layers.Embedding(vocab_size,\n                                          emb_dim)\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D() # no training variable so we can\n                                             # use the same layer for each\n                                             # pooling step\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if nb_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training): #the function for getting input and output, boolean training\n        x = self.embedding(inputs)\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters) #axis -1 means last axis\n        merged = self.dense_1(merged)               #the first axis is the batches and the second one being the pooled \n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"9a6a838f-fe0f-48af-8aa8-79faf5317590","_cell_guid":"d1de7c86-e829-4eb6-91ff-bfc9dd6b107d","trusted":true},"cell_type":"code","source":"#Configuration\nVOCAB_SIZE = tokenizer.vocab_size\n\nEMB_DIM = 200\nNB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 2 #len(set(train_labels))\n\nDROPOUT_RATE = 0.2\n\nBATCH_SIZE = 32\nNB_EPOCHS = 5","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"be6d72bb-dad0-4694-95f0-f976c7156dab","_cell_guid":"b4de5f0b-d78a-472b-85dd-251d5ab5baf8","trusted":true},"cell_type":"code","source":"#Training\nDcnn = DCNN(vocab_size=VOCAB_SIZE,\n            emb_dim=EMB_DIM,\n            nb_filters=NB_FILTERS,\n            FFN_units=FFN_UNITS,\n            nb_classes=NB_CLASSES,\n            dropout_rate=DROPOUT_RATE)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"8b3872ab-3b81-420d-8678-7dbf2c3607eb","_cell_guid":"eec79734-e80b-4dac-81fb-a72bfd5326e1","trusted":true},"cell_type":"code","source":"#Compiling the model\nif NB_CLASSES == 2: #0 or 1\n    Dcnn.compile(loss=\"binary_crossentropy\", #binary classification\n                 optimizer=\"adam\",           \n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\", # n classes output vector of dim n\n                 optimizer=\"adam\",\n                 metrics=[\"sparse_categorical_accuracy\"])","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"462586b9-6251-41a5-8307-a17392fb0907","_cell_guid":"5b832b29-0c6f-44f3-a750-3568122971c3","trusted":true},"cell_type":"code","source":"#Storing the weights\nweights_path = \"../input/sentiments140/\"\n\nweights = tf.train.Checkpoint(Dcnn=Dcnn) #model saved\n\nweights_manager = tf.train.CheckpointManager(weights, weights_path, max_to_keep=5) #file saving the weights\n\nif weights_manager.latest_checkpoint:\n    weights.restore(weights_manager.latest_checkpoint)\n    print(\"Latest weights restored!!\")","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"41cf53cf-0263-443a-8adf-e62159d97742","_cell_guid":"0bbc3fc1-4efd-439f-921a-fc26028b0094","trusted":true,"collapsed":true},"cell_type":"code","source":"#calling the training function\nDcnn.fit(train_inputs,\n         train_labels,\n         batch_size=BATCH_SIZE,\n         epochs=NB_EPOCHS)\nweights_manager.save()","execution_count":23,"outputs":[{"output_type":"stream","text":"Train on 1584076 samples\nEpoch 1/5\n1459552/1584076 [==========================>...] - ETA: 12:52 - loss: 0.4001 - accuracy: 0.8192","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-babfadf064a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          epochs=NB_EPOCHS)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mweights_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"_uuid":"b58440bd-d8cd-4e7c-bdc6-70b3708dd407","_cell_guid":"d161afa4-5323-44dd-96ac-3bb77c0315aa","trusted":true},"cell_type":"code","source":"#Evaluation\n#Testing on the Test data\nresults = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\nprint(results)","execution_count":24,"outputs":[{"output_type":"stream","text":"16000/16000 [==============================] - 5s 338us/sample - loss: 0.3737 - accuracy: 0.8321\n[0.37365066885948184, 0.832125]\n","name":"stdout"}]},{"metadata":{"_uuid":"6df4d408-f9c1-4295-ac61-5d0aacb8a167","_cell_guid":"7a7afa8a-77a4-45e7-a41c-dbfaa6090374","trusted":true},"cell_type":"code","source":"#testing on unseen data\nDcnn(np.array([tokenizer.encode(\"i dont like my job\")]), training=False).numpy()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array([[0.95254475]], dtype=float32)"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}